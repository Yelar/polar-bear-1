{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongBench v2 Evaluation Demo\n",
    "\n",
    "A mini version of The Token Company LongBench v2 experiment on OpenRouter.\n",
    "\n",
    "**Features:**\n",
    "- Multiple compression conditions (baseline, cutoff=0.3, cutoff=0.9)\n",
    "- Budget-guarded execution (default $10 limit)\n",
    "- Deterministic sampling with stratification\n",
    "- Bootstrap confidence intervals\n",
    "- Filesystem caching to avoid duplicate API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q requests python-dotenv tiktoken numpy datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add .env to .gitignore (run once)\n",
    "gitignore_content = \"\"\"\n",
    "# Environment variables\n",
    ".env\n",
    "\n",
    "# Cache and results\n",
    "runs/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".ipynb_checkpoints/\n",
    "\"\"\"\n",
    "\n",
    "with open('.gitignore', 'a') as f:\n",
    "    f.write(gitignore_content)\n",
    "print(\"Updated .gitignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, using environment variables directly\")\n",
    "\n",
    "# Check API key\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if API_KEY:\n",
    "    print(f\"API Key found: {API_KEY[:8]}...{API_KEY[-4:]}\")\n",
    "else:\n",
    "    print(\"WARNING: OPENROUTER_API_KEY not found!\")\n",
    "    print(\"Create a .env file with: OPENROUTER_API_KEY=your_key_here\")\n",
    "    print(\"Or set the environment variable directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Compressor Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from the evals package\n",
    "from evals import compress_text, compress_messages, count_tokens, identity_compressor\n",
    "\n",
    "print(\"Compressor module loaded!\")\n",
    "print(f\"Available functions: compress_text, compress_messages, count_tokens, identity_compressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Demo: Compress One LongBench Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one example from LongBench v2\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading LongBench v2 dataset (streaming)...\")\n",
    "dataset = load_dataset(\"zai-org/LongBench-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "# Get first example\n",
    "example = next(iter(dataset))\n",
    "\n",
    "print(f\"\\nExample ID: {example.get('_id', 'N/A')}\")\n",
    "print(f\"Domain: {example.get('domain', 'N/A')}\")\n",
    "print(f\"Length category: {example.get('length', 'N/A')}\")\n",
    "print(f\"Question: {example.get('question', 'N/A')[:100]}...\")\n",
    "print(f\"\\nChoices:\")\n",
    "print(f\"  A) {example.get('choice_A', 'N/A')[:50]}...\")\n",
    "print(f\"  B) {example.get('choice_B', 'N/A')[:50]}...\")\n",
    "print(f\"  C) {example.get('choice_C', 'N/A')[:50]}...\")\n",
    "print(f\"  D) {example.get('choice_D', 'N/A')[:50]}...\")\n",
    "print(f\"\\nCorrect Answer: {example.get('answer', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context and count tokens\n",
    "context = example.get('context', '')\n",
    "original_tokens = count_tokens(context)\n",
    "\n",
    "print(f\"Context length: {len(context):,} characters\")\n",
    "print(f\"Context tokens: {original_tokens:,} tokens\")\n",
    "print(f\"\\nContext preview (first 500 chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(context[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress with different cutoffs\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPRESSION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cutoffs = [0.0, 0.3, 0.5, 0.9]\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    if cutoff == 0.0:\n",
    "        compressed, stats = identity_compressor(context)\n",
    "        label = \"baseline (no compression)\"\n",
    "    else:\n",
    "        compressed, stats = compress_text(context, importance_cutoff=cutoff)\n",
    "        label = f\"cutoff={cutoff}\"\n",
    "    \n",
    "    orig = stats['original_tokens']\n",
    "    comp = stats['compressed_tokens']\n",
    "    reduction = stats['reduction_ratio']\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Original: {orig:,} tokens\")\n",
    "    print(f\"  Compressed: {comp:,} tokens\")\n",
    "    print(f\"  Reduction: {reduction:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show before/after for cutoff=0.5\n",
    "compressed_05, stats_05 = compress_text(context, importance_cutoff=0.5)\n",
    "\n",
    "print(\"BEFORE (first 300 chars):\")\n",
    "print(\"-\" * 40)\n",
    "print(context[:300])\n",
    "\n",
    "print(\"\\nAFTER compression (cutoff=0.5, first 300 chars):\")\n",
    "print(\"-\" * 40)\n",
    "print(compressed_05[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to Run Evaluation\n",
    "\n",
    "### CLI Usage\n",
    "\n",
    "```bash\n",
    "# Full run with default settings\n",
    "python -m evals.longbench_eval --model openai/gpt-4o-mini --n 30 --seed 42 --cutoffs 0.3 0.9 --budget-usd 10\n",
    "\n",
    "# Dry run (no API calls)\n",
    "python -m evals.longbench_eval --n 10 --no-api\n",
    "\n",
    "# Custom pricing\n",
    "python -m evals.longbench_eval --price-input-per-million 0.15 --price-output-per-million 0.60\n",
    "\n",
    "# Custom cache directory\n",
    "python -m evals.longbench_eval --cache-dir my_cache --results-dir my_results\n",
    "```\n",
    "\n",
    "### Available Flags\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `--model` | `openai/gpt-4o-mini` | OpenRouter model ID |\n",
    "| `--n` | 30 | Number of examples to sample |\n",
    "| `--seed` | 42 | Random seed for reproducibility |\n",
    "| `--cutoffs` | 0.3 0.9 | Compression cutoff values |\n",
    "| `--budget-usd` | 10.0 | Maximum budget in USD |\n",
    "| `--max-context-tokens-for-sampling` | 60000 | Filter contexts to this limit |\n",
    "| `--max-output-tokens` | 8 | Max output tokens from model |\n",
    "| `--price-input-per-million` | 0.15 | Input token price |\n",
    "| `--price-output-per-million` | 0.60 | Output token price |\n",
    "| `--cache-dir` | runs/cache | Cache directory |\n",
    "| `--results-dir` | runs | Results directory |\n",
    "| `--no-api` | False | Dry run mode |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run with N=3 (no API calls needed)\n",
    "print(\"Running dry-run evaluation (N=3, no API calls)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from evals import run_experiment\n",
    "\n",
    "dry_results = run_experiment(\n",
    "    compressor_fn=compress_text,\n",
    "    cutoffs=[0.3, 0.9],\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    n=3,\n",
    "    seed=42,\n",
    "    budget_usd=10.0,\n",
    "    no_api=True,  # No API calls\n",
    ")\n",
    "\n",
    "print(\"\\nDry run complete!\")\n",
    "print(f\"Conditions tested: {len(dry_results.get('conditions', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If API key is available, run a small real evaluation\n",
    "if API_KEY:\n",
    "    print(\"API key found! Running small evaluation (N=10)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = run_experiment(\n",
    "        compressor_fn=compress_text,\n",
    "        cutoffs=[0.3, 0.9],\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        n=10,\n",
    "        seed=42,\n",
    "        budget_usd=10.0,\n",
    "        no_api=False,\n",
    "        openrouter_api_key=API_KEY,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTotal cost: ${results.get('total_cost', 0):.4f}\")\n",
    "else:\n",
    "    print(\"No API key found. Skipping real evaluation.\")\n",
    "    print(\"To run with real API calls:\")\n",
    "    print(\"1. Create a .env file with OPENROUTER_API_KEY=your_key\")\n",
    "    print(\"2. Re-run this cell\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load results from file (if exists)\n",
    "results_file = Path(\"runs/results.json\")\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        saved_results = json.load(f)\n",
    "    print(\"Loaded results from runs/results.json\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    config = saved_results.get('config', {})\n",
    "    print(f\"  Model: {config.get('model', 'N/A')}\")\n",
    "    print(f\"  N examples: {config.get('n', 'N/A')}\")\n",
    "    print(f\"  Seed: {config.get('seed', 'N/A')}\")\n",
    "    print(f\"  Budget: ${config.get('budget_usd', 'N/A')}\")\n",
    "    print(f\"\\nTotal cost: ${saved_results.get('total_cost', 0):.4f}\")\n",
    "else:\n",
    "    print(\"No results file found. Run an evaluation first.\")\n",
    "    saved_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "if saved_results and 'conditions' in saved_results:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"RESULTS TABLE\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Condition':<15} | {'Accuracy':>8} | {'Delta':>8} | {'95% CI':>20} | {'Token Red.':>10} | {'Invalid':>8} | {'Cost':>8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for cond in saved_results['conditions']:\n",
    "        name = cond['condition']\n",
    "        acc = cond['accuracy']\n",
    "        delta = cond['delta_vs_baseline']\n",
    "        ci_low = cond['delta_ci_lower']\n",
    "        ci_high = cond['delta_ci_upper']\n",
    "        token_red = cond['token_reduction_vs_baseline']\n",
    "        invalid = cond['invalid_rate']\n",
    "        cost = cond['total_cost']\n",
    "        \n",
    "        if name == 'baseline':\n",
    "            ci_str = \"---\"\n",
    "            delta_str = \"---\"\n",
    "        else:\n",
    "            ci_str = f\"[{ci_low:+.3f}, {ci_high:+.3f}]\"\n",
    "            delta_str = f\"{delta:+.3f}\"\n",
    "        \n",
    "        print(f\"{name:<15} | {acc:>8.3f} | {delta_str:>8} | {ci_str:>20} | {token_red:>9.1%} | {invalid:>7.1%} | ${cost:>7.4f}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs condition\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if saved_results and 'conditions' in saved_results:\n",
    "    conditions = saved_results['conditions']\n",
    "    \n",
    "    # Extract data\n",
    "    names = [c['condition'] for c in conditions]\n",
    "    accuracies = [c['accuracy'] for c in conditions]\n",
    "    token_reductions = [c['token_reduction_vs_baseline'] * 100 for c in conditions]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Accuracy by condition\n",
    "    colors = ['#2ecc71' if n == 'baseline' else '#3498db' for n in names]\n",
    "    bars1 = ax1.bar(names, accuracies, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    ax1.set_xlabel('Condition', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_title('Accuracy by Compression Condition', fontsize=14)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axhline(y=accuracies[0], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{acc:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Token reduction\n",
    "    bars2 = ax2.bar(names, token_reductions, color='#e74c3c', edgecolor='black', linewidth=1.2)\n",
    "    ax2.set_xlabel('Condition', fontsize=12)\n",
    "    ax2.set_ylabel('Token Reduction (%)', fontsize=12)\n",
    "    ax2.set_title('Token Reduction vs Baseline', fontsize=14)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, red in zip(bars2, token_reductions):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{red:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('runs/accuracy_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPlot saved to runs/accuracy_plot.png\")\n",
    "else:\n",
    "    print(\"No results to plot. Run an evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy-Token Tradeoff Plot\n",
    "if saved_results and 'conditions' in saved_results:\n",
    "    conditions = saved_results['conditions']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    for cond in conditions:\n",
    "        name = cond['condition']\n",
    "        acc = cond['accuracy']\n",
    "        tokens = cond['mean_input_tokens']\n",
    "        \n",
    "        color = '#2ecc71' if name == 'baseline' else '#3498db'\n",
    "        marker = 'o' if name == 'baseline' else 's'\n",
    "        \n",
    "        ax.scatter(tokens, acc, s=200, c=color, marker=marker, \n",
    "                  edgecolors='black', linewidth=1.5, label=name, zorder=5)\n",
    "        ax.annotate(name, (tokens, acc), xytext=(10, 10), \n",
    "                   textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel('Mean Input Tokens', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Accuracy vs Token Count Trade-off', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('runs/tradeoff_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Plot saved to runs/tradeoff_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Compression Module** (`evals/compressor.py`)\n",
    "   - `compress_text(prompt, importance_cutoff)` - Compress raw text\n",
    "   - `compress_messages(messages, importance_cutoff)` - Compress chat messages\n",
    "   - `count_tokens(text)` - Count tokens using tiktoken\n",
    "\n",
    "2. **Evaluation Module** (`evals/longbench_eval.py`)\n",
    "   - `run_experiment(...)` - Run full evaluation programmatically\n",
    "   - CLI: `python -m evals.longbench_eval --n 30 --cutoffs 0.3 0.9`\n",
    "\n",
    "3. **Key Features**\n",
    "   - Budget-guarded execution (auto-reduces N if over budget)\n",
    "   - Filesystem caching (no duplicate API calls)\n",
    "   - Bootstrap confidence intervals\n",
    "   - Stratified sampling by domain and length\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "```\n",
    ".\n",
    "├── evals/\n",
    "│   ├── __init__.py\n",
    "│   ├── compressor.py\n",
    "│   └── longbench_eval.py\n",
    "├── model.ipynb\n",
    "├── .env.example\n",
    "├── requirements.txt\n",
    "└── runs/\n",
    "    ├── cache/\n",
    "    ├── results.json\n",
    "    └── results.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
