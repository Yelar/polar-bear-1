{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b6b9d354",
      "metadata": {},
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7d4b2fe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "\n",
        "def _pip_install(pkgs):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "\n",
        "# Install core dependencies (best-effort)\n",
        "_packages = [\n",
        "    \"transformers>=4.36.0\",\n",
        "    \"sentence-transformers>=2.2.2\",\n",
        "    \"rank-bm25>=0.2.2\",\n",
        "    \"tiktoken>=0.5.1\",\n",
        "    \"spacy>=3.7.2\",\n",
        "]\n",
        "for _pkg in _packages:\n",
        "    try:\n",
        "        _pip_install([_pkg])\n",
        "    except Exception as e:\n",
        "        print(f\"pip install failed for {_pkg}: {e}\")\n",
        "\n",
        "# Determinism\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb48d0e8",
      "metadata": {},
      "source": [
        "# Compressor Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "341d106f",
      "metadata": {},
      "source": [
        "Compression notes:\n",
        "- `importance_cutoff` is the fraction of lowest-surprisal tokens to remove (higher means more aggressive).\n",
        "- Surprisal pruning operates at the LM token (BPE) level to preserve spacing and punctuation.\n",
        "- JSON/YAML and code blocks are never pruned; they are only selected or dropped as chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3d3afa60",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Optional token counter\n",
        "try:\n",
        "    import tiktoken\n",
        "    _tiktoken = tiktoken.get_encoding(\"cl100k_base\")\n",
        "except Exception:\n",
        "    _tiktoken = None\n",
        "\n",
        "# Lazy-loaded models\n",
        "_lm_tokenizer = None\n",
        "_lm_model = None\n",
        "_embedder = None\n",
        "_spacy_nlp = None\n",
        "\n",
        "\n",
        "def get_lm():\n",
        "    global _lm_tokenizer, _lm_model\n",
        "    if _lm_tokenizer is None or _lm_model is None:\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "        _lm_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "        _lm_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "        _lm_model.eval()\n",
        "    return _lm_tokenizer, _lm_model\n",
        "\n",
        "\n",
        "def get_embedder():\n",
        "    global _embedder\n",
        "    if _embedder is None:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        _embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    return _embedder\n",
        "\n",
        "\n",
        "def get_spacy_nlp():\n",
        "    global _spacy_nlp\n",
        "    if _spacy_nlp is None:\n",
        "        try:\n",
        "            import spacy\n",
        "            _spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            _spacy_nlp = None\n",
        "    return _spacy_nlp\n",
        "\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    if _tiktoken is not None:\n",
        "        return len(_tiktoken.encode(text))\n",
        "    try:\n",
        "        tokenizer, _ = get_lm()\n",
        "        return len(tokenizer.encode(text))\n",
        "    except Exception:\n",
        "        return max(1, len(text.split()))\n",
        "\n",
        "\n",
        "def split_code_fences(text: str) -> List[Dict[str, str]]:\n",
        "    pattern = re.compile(r\"```.*?```\", re.DOTALL)\n",
        "    segments = []\n",
        "    last = 0\n",
        "    for m in pattern.finditer(text):\n",
        "        if m.start() > last:\n",
        "            segments.append({\"type\": \"text\", \"text\": text[last:m.start()]})\n",
        "        segments.append({\"type\": \"code\", \"text\": text[m.start():m.end()]})\n",
        "        last = m.end()\n",
        "    if last < len(text):\n",
        "        segments.append({\"type\": \"text\", \"text\": text[last:]})\n",
        "    return segments\n",
        "\n",
        "\n",
        "def is_json_like(text: str) -> bool:\n",
        "    s = text.strip()\n",
        "    if not s:\n",
        "        return False\n",
        "    if s[0] in \"[{\":\n",
        "        try:\n",
        "            json.loads(s)\n",
        "            return True\n",
        "        except Exception:\n",
        "            if re.search(r\"\\\"[^\\\"]+\\\"\\s*:\", s):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_yaml_like(text: str) -> bool:\n",
        "    s = text.strip()\n",
        "    if not s or s[0] in \"[{\":\n",
        "        return False\n",
        "    lines = [ln for ln in s.splitlines() if ln.strip()]\n",
        "    if len(lines) < 2:\n",
        "        return False\n",
        "    key_like = 0\n",
        "    for ln in lines:\n",
        "        if re.match(r\"^[\\w\\-\\s]+:\\s*\", ln):\n",
        "            key_like += 1\n",
        "    return key_like >= max(2, len(lines) // 2)\n",
        "\n",
        "\n",
        "def split_into_chunks(text: str) -> List[str]:\n",
        "    parts = [p for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    if len(parts) <= 1:\n",
        "        sentences = [s for s in re.split(r\"(?<=[.!?])\\s+\", text.strip()) if s.strip()]\n",
        "        return sentences if sentences else [text]\n",
        "    return parts\n",
        "\n",
        "\n",
        "def extract_keywords(question: str, max_keywords: int = 10) -> List[str]:\n",
        "    nlp = get_spacy_nlp()\n",
        "    keywords = []\n",
        "    if nlp:\n",
        "        doc = nlp(question)\n",
        "        for ent in doc.ents:\n",
        "            keywords.append(ent.text)\n",
        "        for chunk in doc.noun_chunks:\n",
        "            keywords.append(chunk.text)\n",
        "    else:\n",
        "        tokens = re.findall(r\"[A-Za-z0-9\\-/_.]+\", question)\n",
        "        stop = {\n",
        "            \"the\", \"a\", \"an\", \"and\", \"or\", \"of\", \"to\", \"in\", \"for\", \"on\", \"with\",\n",
        "            \"what\", \"which\", \"who\", \"when\", \"where\", \"why\", \"how\", \"is\", \"are\", \"was\",\n",
        "            \"were\", \"be\", \"by\", \"from\", \"that\", \"this\", \"it\", \"as\", \"at\"\n",
        "        }\n",
        "        for t in tokens:\n",
        "            if t.lower() not in stop and len(t) > 2:\n",
        "                keywords.append(t)\n",
        "    # Normalize and dedupe\n",
        "    cleaned = []\n",
        "    seen = set()\n",
        "    for kw in keywords:\n",
        "        kw_norm = kw.strip().lower()\n",
        "        if kw_norm and kw_norm not in seen:\n",
        "            seen.add(kw_norm)\n",
        "            cleaned.append(kw.strip())\n",
        "    return cleaned[:max_keywords]\n",
        "\n",
        "\n",
        "def embed_texts(texts: List[str]) -> np.ndarray:\n",
        "    embedder = get_embedder()\n",
        "    emb = embedder.encode(texts, normalize_embeddings=True)\n",
        "    return np.asarray(emb)\n",
        "\n",
        "\n",
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))\n",
        "\n",
        "\n",
        "def mmr_select(embeddings: np.ndarray, query_embedding: np.ndarray, token_counts: List[int],\n",
        "               budget: int, mmr_lambda: float) -> List[int]:\n",
        "    selected = []\n",
        "    if len(embeddings) == 0 or budget <= 0:\n",
        "        return selected\n",
        "\n",
        "    sims = embeddings @ query_embedding\n",
        "    remaining = set(range(len(embeddings)))\n",
        "    total = 0\n",
        "\n",
        "    while remaining:\n",
        "        best_idx = None\n",
        "        best_score = -1e9\n",
        "        for i in sorted(remaining):\n",
        "            if total + token_counts[i] > budget:\n",
        "                continue\n",
        "            max_sim = 0.0\n",
        "            if selected:\n",
        "                max_sim = max(cosine_sim(embeddings[i], embeddings[j]) for j in selected)\n",
        "            score = mmr_lambda * sims[i] - (1.0 - mmr_lambda) * max_sim\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_idx = i\n",
        "        if best_idx is None:\n",
        "            break\n",
        "        selected.append(best_idx)\n",
        "        remaining.remove(best_idx)\n",
        "        total += token_counts[best_idx]\n",
        "    return selected\n",
        "\n",
        "\n",
        "def find_protected_spans(text: str) -> List[Tuple[int, int]]:\n",
        "    spans = []\n",
        "\n",
        "    def add_spans(pattern, flags=0):\n",
        "        for m in re.finditer(pattern, text, flags):\n",
        "            spans.append((m.start(), m.end()))\n",
        "\n",
        "    # URLs\n",
        "    add_spans(r\"https?://\\S+\")\n",
        "    # File paths\n",
        "    add_spans(r\"(?:[A-Za-z]:\\\\|/)[\\w\\-._~/:\\\\]+\")\n",
        "    # Numbers / dates / IDs\n",
        "    add_spans(r\"\\b\\d+(?:[\\-/:]\\d+)*\\b\")\n",
        "    # Quoted strings\n",
        "    add_spans(r\"(['\\\"])(?:(?=(\\\\?))\\2.)*?\\1\")\n",
        "    # Negations and critical operators\n",
        "    negations = [\n",
        "        \"not\", \"never\", \"no\", \"without\", \"except\", \"unless\", \"must\", \"mustn't\",\n",
        "        \"don't\", \"do not\", \"can't\", \"cannot\", \"at least\", \"at most\"\n",
        "    ]\n",
        "    for term in negations:\n",
        "        add_spans(r\"\\b\" + re.escape(term) + r\"\\b\", flags=re.IGNORECASE)\n",
        "\n",
        "    # Headings / section delimiters\n",
        "    idx = 0\n",
        "    for line in text.splitlines(True):\n",
        "        stripped = line.strip()\n",
        "        if re.match(r\"^#{1,6}\\s+\\S+\", stripped) or stripped in {\"---\", \"===\", \"***\"}:\n",
        "            spans.append((idx, idx + len(line)))\n",
        "        idx += len(line)\n",
        "\n",
        "    return spans\n",
        "\n",
        "\n",
        "def token_char_spans(text: str, tokenizer) -> Tuple[List[int], List[Tuple[int, int]]]:\n",
        "    ids = tokenizer.encode(text)\n",
        "    spans = []\n",
        "    built = \"\"\n",
        "    for tid in ids:\n",
        "        token_str = tokenizer.decode([tid])\n",
        "        start = len(built)\n",
        "        built += token_str\n",
        "        end = len(built)\n",
        "        spans.append((start, end))\n",
        "    return ids, spans\n",
        "\n",
        "\n",
        "def compute_surprisals(text: str) -> List[float]:\n",
        "    tokenizer, model = get_lm()\n",
        "    ids = tokenizer.encode(text)\n",
        "    if len(ids) <= 1:\n",
        "        return [0.0] * len(ids)\n",
        "\n",
        "    max_len = getattr(model.config, \"n_positions\", 1024)\n",
        "    stride = max_len // 2\n",
        "\n",
        "    surprisals = [None] * len(ids)\n",
        "\n",
        "    if len(ids) <= max_len:\n",
        "        input_ids = torch.tensor([ids])\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids).logits[0]\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "        surprisals[0] = 0.0\n",
        "        for i in range(1, len(ids)):\n",
        "            surpr = -log_probs[i - 1, ids[i]].item()\n",
        "            surprisals[i] = surpr\n",
        "        return [float(x) for x in surprisals]\n",
        "\n",
        "    for start in range(0, len(ids), stride):\n",
        "        end = min(start + max_len, len(ids))\n",
        "        segment = ids[start:end]\n",
        "        input_ids = torch.tensor([segment])\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids).logits[0]\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "        for i in range(1, len(segment)):\n",
        "            idx = start + i\n",
        "            if surprisals[idx] is None:\n",
        "                surprisals[idx] = -log_probs[i - 1, segment[i]].item()\n",
        "    for i, val in enumerate(surprisals):\n",
        "        if val is None:\n",
        "            surprisals[i] = 0.0\n",
        "    return [float(x) for x in surprisals]\n",
        "\n",
        "\n",
        "def prune_text_with_surprisal(text: str, importance_cutoff: float, target_reduction: Optional[float]) -> Tuple[str, Dict]:\n",
        "    tokenizer, _ = get_lm()\n",
        "    ids, spans = token_char_spans(text, tokenizer)\n",
        "    if not ids:\n",
        "        return text, {\"removed_tokens\": 0, \"kept_tokens\": 0}\n",
        "\n",
        "    surprisal = compute_surprisals(text)\n",
        "    protected_spans = find_protected_spans(text)\n",
        "\n",
        "    def is_protected(span):\n",
        "        for s, e in protected_spans:\n",
        "            if not (span[1] <= s or span[0] >= e):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    candidates = []\n",
        "    for i, sp in enumerate(spans):\n",
        "        if not is_protected(sp):\n",
        "            candidates.append(i)\n",
        "\n",
        "    if not candidates:\n",
        "        return text, {\"removed_tokens\": 0, \"kept_tokens\": len(ids)}\n",
        "\n",
        "    # Determine how many tokens to remove based on cutoff and target\n",
        "    candidates_sorted = sorted(candidates, key=lambda i: surprisal[i])\n",
        "    remove_by_cutoff = int(len(candidates_sorted) * importance_cutoff)\n",
        "    remove_by_target = remove_by_cutoff\n",
        "    if target_reduction is not None:\n",
        "        remove_by_target = min(remove_by_cutoff, int(len(ids) * target_reduction))\n",
        "    remove_count = max(0, min(remove_by_target, len(candidates_sorted)))\n",
        "\n",
        "    to_remove = set(candidates_sorted[:remove_count])\n",
        "    kept_ids = [tid for i, tid in enumerate(ids) if i not in to_remove]\n",
        "\n",
        "    pruned_text = tokenizer.decode(kept_ids)\n",
        "    return pruned_text, {\n",
        "        \"removed_tokens\": len(to_remove),\n",
        "        \"kept_tokens\": len(kept_ids),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7ed77ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ChunkInfo:\n",
        "    text: str\n",
        "    kind: str  # text, code, json\n",
        "    order: int\n",
        "    token_count: int\n",
        "    prunable: bool\n",
        "\n",
        "\n",
        "def compress_text_block(text: str, query_text: str, importance_cutoff: float, target_reduction: Optional[float],\n",
        "                         block_budget: Optional[int], mmr_lambda: float) -> Tuple[str, Dict]:\n",
        "    segments = split_code_fences(text)\n",
        "\n",
        "    chunks = []\n",
        "    order = 0\n",
        "    for seg in segments:\n",
        "        if seg[\"type\"] == \"code\":\n",
        "            chunks.append(ChunkInfo(\n",
        "                text=seg[\"text\"],\n",
        "                kind=\"code\",\n",
        "                order=order,\n",
        "                token_count=count_tokens(seg[\"text\"]),\n",
        "                prunable=False,\n",
        "            ))\n",
        "            order += 1\n",
        "            continue\n",
        "\n",
        "        for part in split_into_chunks(seg[\"text\"]):\n",
        "            kind = \"json\" if (is_json_like(part) or is_yaml_like(part)) else \"text\"\n",
        "            chunks.append(ChunkInfo(\n",
        "                text=part,\n",
        "                kind=kind,\n",
        "                order=order,\n",
        "                token_count=count_tokens(part),\n",
        "                prunable=(kind == \"text\"),\n",
        "            ))\n",
        "            order += 1\n",
        "\n",
        "    if not chunks:\n",
        "        return text, {\"selected_chunks\": 0, \"chunk_selection_time\": 0.0, \"surprisal_time\": 0.0}\n",
        "\n",
        "    forced = [i for i, ch in enumerate(chunks) if ch.kind == \"code\"]\n",
        "    forced_tokens = sum(chunks[i].token_count for i in forced)\n",
        "\n",
        "    if block_budget is None:\n",
        "        block_budget = sum(ch.token_count for ch in chunks)\n",
        "    remaining_budget = max(0, block_budget - forced_tokens)\n",
        "\n",
        "    # Prepare candidate chunks (non-code)\n",
        "    candidate_idxs = [i for i, ch in enumerate(chunks) if ch.kind != \"code\"]\n",
        "    candidate_texts = [chunks[i].text for i in candidate_idxs]\n",
        "    candidate_tokens = [chunks[i].token_count for i in candidate_idxs]\n",
        "\n",
        "    selected_idxs = set(forced)\n",
        "\n",
        "    # Keyword coverage\n",
        "    keywords = extract_keywords(query_text)\n",
        "    chunk_selection_time = 0.0\n",
        "    if candidate_texts and keywords:\n",
        "        t_embed = time.time()\n",
        "        embeddings = embed_texts(candidate_texts)\n",
        "        query_emb = embed_texts([query_text])[0]\n",
        "        # Force include chunks that cover keywords\n",
        "        for kw in keywords:\n",
        "            matches = [j for j, txt in enumerate(candidate_texts) if kw.lower() in txt.lower()]\n",
        "            if not matches:\n",
        "                continue\n",
        "            # Prefer the most relevant match\n",
        "            best_local = max(matches, key=lambda j: float(embeddings[j] @ query_emb))\n",
        "            global_idx = candidate_idxs[best_local]\n",
        "            if global_idx not in selected_idxs:\n",
        "                if chunks[global_idx].token_count <= remaining_budget:\n",
        "                    selected_idxs.add(global_idx)\n",
        "                    remaining_budget -= chunks[global_idx].token_count\n",
        "        chunk_selection_time = time.time() - t_embed\n",
        "    else:\n",
        "        embeddings = np.zeros((len(candidate_texts), 1))\n",
        "        query_emb = np.zeros((1,))\n",
        "\n",
        "    # MMR selection\n",
        "    if candidate_texts and remaining_budget > 0:\n",
        "        t_mmr = time.time()\n",
        "        mmr_selected_local = mmr_select(\n",
        "            embeddings,\n",
        "            query_emb,\n",
        "            candidate_tokens,\n",
        "            remaining_budget,\n",
        "            mmr_lambda,\n",
        "        )\n",
        "        for local_idx in mmr_selected_local:\n",
        "            selected_idxs.add(candidate_idxs[local_idx])\n",
        "        chunk_selection_time += time.time() - t_mmr\n",
        "\n",
        "    # Assemble in order\n",
        "    selected_chunks = [ch for i, ch in enumerate(chunks) if i in selected_idxs]\n",
        "    selected_chunks.sort(key=lambda c: c.order)\n",
        "\n",
        "    t_surprisal = time.time()\n",
        "    out_parts = []\n",
        "    for ch in selected_chunks:\n",
        "        if ch.prunable:\n",
        "            pruned, _ = prune_text_with_surprisal(ch.text, importance_cutoff, target_reduction)\n",
        "            out_parts.append(pruned)\n",
        "        else:\n",
        "            out_parts.append(ch.text)\n",
        "    surprisal_time = time.time() - t_surprisal\n",
        "\n",
        "    compressed_text = \"\\n\\n\".join([p for p in out_parts if p.strip()])\n",
        "\n",
        "    stats = {\n",
        "        \"selected_chunks\": len(selected_chunks),\n",
        "        \"chunk_selection_time\": chunk_selection_time,\n",
        "        \"surprisal_time\": surprisal_time,\n",
        "    }\n",
        "    return compressed_text, stats\n",
        "\n",
        "\n",
        "def is_tool_schema(msg: Dict) -> bool:\n",
        "    if msg.get(\"role\") == \"tool\":\n",
        "        return True\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    if isinstance(content, str) and \"\\\"properties\\\"\" in content and \"schema\" in content.lower():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def compress_messages(messages: List[Dict], importance_cutoff: float = 0.3,\n",
        "                      target_reduction: Optional[float] = None,\n",
        "                      max_context_tokens: Optional[int] = None,\n",
        "                      recent_messages_to_keep: int = 4,\n",
        "                      mmr_lambda: float = 0.7) -> Tuple[List[Dict], Dict]:\n",
        "    start_total = time.time()\n",
        "    min_compress_tokens = 120\n",
        "\n",
        "    if not messages:\n",
        "        return [], {\"original_token_count\": 0, \"compressed_token_count\": 0, \"reduction_pct\": 0.0}\n",
        "\n",
        "    # Identify latest user question\n",
        "    latest_user_idx = max((i for i, m in enumerate(messages) if m.get(\"role\") == \"user\"), default=-1)\n",
        "\n",
        "    always_keep = set()\n",
        "    for i, m in enumerate(messages):\n",
        "        if m.get(\"role\") in {\"system\", \"developer\"}:\n",
        "            always_keep.add(i)\n",
        "        if i == latest_user_idx:\n",
        "            always_keep.add(i)\n",
        "        if is_tool_schema(m):\n",
        "            always_keep.add(i)\n",
        "\n",
        "    recent_keep_start = max(0, len(messages) - recent_messages_to_keep)\n",
        "    recent_keep = set(range(recent_keep_start, len(messages)))\n",
        "\n",
        "    compressible = [\n",
        "        i for i in range(len(messages))\n",
        "        if i not in always_keep and i not in recent_keep\n",
        "    ]\n",
        "    # Only compress large blocks to preserve structure\n",
        "    compressible = [i for i in compressible if count_tokens(messages[i].get(\"content\", \"\")) >= min_compress_tokens]\n",
        "\n",
        "\n",
        "    instruction_summary = \"\\n\".join(\n",
        "        m.get(\"content\", \"\") for i, m in enumerate(messages) if i in always_keep and m.get(\"role\") in {\"system\", \"developer\"}\n",
        "    )\n",
        "    question_text = messages[latest_user_idx].get(\"content\", \"\") if latest_user_idx >= 0 else \"\"\n",
        "    query_text = (instruction_summary + \"\\n\" + question_text).strip()\n",
        "\n",
        "    original_token_count = sum(count_tokens(m.get(\"content\", \"\")) for m in messages)\n",
        "\n",
        "    reserved_tokens = sum(count_tokens(messages[i].get(\"content\", \"\")) for i in range(len(messages)) if i not in compressible)\n",
        "    compressible_tokens = sum(count_tokens(messages[i].get(\"content\", \"\")) for i in compressible)\n",
        "\n",
        "    block_budgets = {}\n",
        "    if max_context_tokens is not None:\n",
        "        budget_for_compressible = max(0, max_context_tokens - reserved_tokens)\n",
        "        for i in compressible:\n",
        "            blk_tokens = count_tokens(messages[i].get(\"content\", \"\"))\n",
        "            if compressible_tokens > 0:\n",
        "                blk_budget = int(budget_for_compressible * (blk_tokens / compressible_tokens))\n",
        "            else:\n",
        "                blk_budget = 0\n",
        "            block_budgets[i] = max(0, blk_budget)\n",
        "    else:\n",
        "        for i in compressible:\n",
        "            blk_tokens = count_tokens(messages[i].get(\"content\", \"\"))\n",
        "            if target_reduction is None:\n",
        "                block_budgets[i] = blk_tokens\n",
        "            else:\n",
        "                block_budgets[i] = max(0, int(blk_tokens * (1.0 - target_reduction)))\n",
        "\n",
        "    compressed_messages = []\n",
        "    per_block = []\n",
        "    total_chunk_time = 0.0\n",
        "    total_surprisal_time = 0.0\n",
        "\n",
        "    for i, msg in enumerate(messages):\n",
        "        content = msg.get(\"content\", \"\")\n",
        "        if i not in compressible:\n",
        "            compressed_messages.append(msg)\n",
        "            continue\n",
        "\n",
        "        budget = block_budgets.get(i, None)\n",
        "        compressed_text, stats = compress_text_block(\n",
        "            content, query_text, importance_cutoff, target_reduction, budget, mmr_lambda\n",
        "        )\n",
        "        total_chunk_time += stats.get(\"chunk_selection_time\", 0.0)\n",
        "        total_surprisal_time += stats.get(\"surprisal_time\", 0.0)\n",
        "\n",
        "        compressed_messages.append({\"role\": msg.get(\"role\"), \"content\": compressed_text})\n",
        "        per_block.append({\n",
        "            \"index\": i,\n",
        "            \"original_tokens\": count_tokens(content),\n",
        "            \"compressed_tokens\": count_tokens(compressed_text),\n",
        "        })\n",
        "\n",
        "    compressed_token_count = sum(count_tokens(m.get(\"content\", \"\")) for m in compressed_messages)\n",
        "    reduction_pct = 0.0\n",
        "    if original_token_count > 0:\n",
        "        reduction_pct = 100.0 * (original_token_count - compressed_token_count) / original_token_count\n",
        "\n",
        "    stats = {\n",
        "        \"original_token_count\": original_token_count,\n",
        "        \"compressed_token_count\": compressed_token_count,\n",
        "        \"reduction_pct\": reduction_pct,\n",
        "        \"per_block\": per_block,\n",
        "        \"timing\": {\n",
        "            \"chunk_selection_time\": total_chunk_time,\n",
        "            \"surprisal_time\": total_surprisal_time,\n",
        "            \"total_time\": time.time() - start_total,\n",
        "        },\n",
        "    }\n",
        "    return compressed_messages, stats\n",
        "\n",
        "\n",
        "def compress_prompt(prompt: str, **kwargs) -> Tuple[str, Dict]:\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    compressed, stats = compress_messages(messages, **kwargs)\n",
        "    return compressed[0][\"content\"], stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5852a065",
      "metadata": {},
      "source": [
        "# Mini Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a71d7d6d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "def _filler_paragraph(seed: int) -> str:\n",
        "    random.seed(seed)\n",
        "    topics = [\n",
        "        \"weather patterns\", \"supply chain notes\", \"historical anecdotes\", \"meeting minutes\",\n",
        "        \"engineering updates\", \"budget discussions\", \"team retrospectives\", \"market research\"\n",
        "    ]\n",
        "    verbs = [\"mentions\", \"discusses\", \"summarizes\", \"explores\", \"compares\", \"lists\"]\n",
        "    objs = [\"multiple initiatives\", \"prior milestones\", \"miscellaneous items\", \"side experiments\", \"legacy systems\"]\n",
        "    return (\n",
        "        f\"This paragraph {random.choice(verbs)} {random.choice(objs)} across {random.choice(topics)} \"\n",
        "        f\"and includes unrelated details, dates like 2021-09-17, and IDs such as TX-{seed:04d}.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_example(i: int) -> Dict:\n",
        "    project = f\"Project-{i}\"\n",
        "    city = random.choice([\"Oslo\", \"Lima\", \"Prague\", \"Cairo\", \"Seoul\", \"Dakar\", \"Riga\", \"Hanoi\"]) + f\"-{i}\"\n",
        "    question = f\"For {project}, what is the designated warehouse city?\"\n",
        "    context_parts = []\n",
        "    for j in range(4):\n",
        "        context_parts.append(_filler_paragraph(i * 10 + j))\n",
        "    context_parts.append(\n",
        "        f\"### Operational Note\\nThe designated warehouse city for {project} is {city}. \"\n",
        "        f\"This must not be changed without approval.\"\n",
        "    )\n",
        "    context_parts.append(_filler_paragraph(i * 10 + 5))\n",
        "    context_parts.append(\n",
        "        \"```python\\n\"\n",
        "        \"# Example code block that should never be removed\\n\"\n",
        "        \"def schedule(route_id):\\n\"\n",
        "        \"    return f\\\"Route {route_id} scheduled\\\"\\n\"\n",
        "        \"```\"\n",
        "    )\n",
        "    context_parts.append(\n",
        "        \"{\\n  \\\"project\\\": \\\"%s\\\",\\n  \\\"owner\\\": \\\"Ops\\\",\\n  \\\"ticket\\\": \\\"TK-%04d\\\"\\n}\" % (project, 1000 + i)\n",
        "    )\n",
        "    context_parts.append(\n",
        "        \"---\\nSection: Miscellaneous\\nThis section includes a URL https://example.com/docs and a path /var/log/syslog.\"\n",
        "    )\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "    answer = city\n",
        "    return {\"question\": question, \"context\": context, \"answer\": answer}\n",
        "\n",
        "\n",
        "examples = [generate_example(i) for i in range(20)]\n",
        "len(examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "635e057b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: load a public dataset if available (not required)\n",
        "# This block is safe to skip if datasets or network access is unavailable.\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    _has_datasets = True\n",
        "except Exception:\n",
        "    _has_datasets = False\n",
        "\n",
        "if _has_datasets:\n",
        "    try:\n",
        "        _sample = load_dataset(\"squad\", split=\"validation[:20]\")\n",
        "        print(\"Loaded optional dataset sample with\", len(_sample), \"examples\")\n",
        "    except Exception as e:\n",
        "        print(\"Optional dataset load failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a21f292e",
      "metadata": {},
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bcfbc7f8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1988f830286942b789ee849aea31ae6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d1e315d8f9c4870958fc99522af2447",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8b2034efdb845aea8de9bfb7dc51974",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5ff408966904a8ebfff4021fb2df87e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f38c07aef184a04ac01d17c34db194e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "173f037e4d8340f6acd486b8d88008f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2365bfb6c43745d8ba68657b46a63853",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f63fa59c98a24887830313f1ae5542d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73b24a93f31841f299312f93d68a1e86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fda5a4e6b1af427d9357a84c6a6e4df4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecaf84bbeb5b41109d0fb7c4f9bf2be7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83f5be2c22d44811b14020043830a2e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aaa71b2abd849c689851dbb282ac209",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4da407c4e5bf46c1818d21b5f73155fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2d1a4f204354293b41734a9ebf35a0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd9c5488ab2b4899b72f4a8e57450848",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60231b64dd804365afcb191c45f3f68a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68b55404968f4b1596b4afaa8f37c189",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cutoff=0.3 | acc_base=1.00 acc_comp=0.15 | avg_tokens 292.4->129.4 | diff_mean=-0.849 CI=(-1.0, -0.7) P(better)=0.00\n",
            "cutoff=0.9 | acc_base=1.00 acc_comp=0.00 | avg_tokens 292.4->118.5 | diff_mean=-1.000 CI=(-1.0, -1.0) P(better)=0.00\n",
            "Note: Small sample size; bootstrap estimates are noisy.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def normalize_answer(text: str) -> str:\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"[^a-z0-9\\s\\-]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def heuristic_qa(question: str, context: str) -> str:\n",
        "    # Simple pattern-based extraction\n",
        "    for pat in [r\"designated warehouse city for .*? is ([A-Za-z0-9\\-]+)\", r\"Answer:\\s*([A-Za-z0-9\\-]+)\"]:\n",
        "        m = re.search(pat, context, flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "    # Fallback: pick the most relevant sentence containing \"warehouse city\"\n",
        "    for line in context.splitlines():\n",
        "        if \"warehouse city\" in line.lower():\n",
        "            m = re.search(r\"is ([A-Za-z0-9\\-]+)\", line)\n",
        "            if m:\n",
        "                return m.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def answer_local(question: str, context: str) -> str:\n",
        "    # Heuristic baseline (fast, no extra models)\n",
        "    return heuristic_qa(question, context)\n",
        "\n",
        "\n",
        "def answer_openai(question: str, context: str) -> Optional[str]:\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "    if not api_key:\n",
        "        return None\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer concisely.\"\n",
        "        resp = client.responses.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            input=prompt,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        return resp.output_text.strip()\n",
        "    except Exception as e:\n",
        "        print(\"OpenAI call failed:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "def evaluate(examples, importance_cutoff, target_reduction=0.4, recent_messages_to_keep=1):\n",
        "    baseline_correct = []\n",
        "    compressed_correct = []\n",
        "    token_counts = []\n",
        "\n",
        "    for ex in examples:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a concise QA assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{ex['context']}\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Question: {ex['question']}\"},\n",
        "        ]\n",
        "\n",
        "        # Baseline\n",
        "        baseline_answer = answer_local(ex[\"question\"], ex[\"context\"])\n",
        "\n",
        "        # Compressed\n",
        "        compressed_msgs, stats = compress_messages(\n",
        "            messages,\n",
        "            importance_cutoff=importance_cutoff,\n",
        "            target_reduction=target_reduction,\n",
        "            max_context_tokens=None,\n",
        "            recent_messages_to_keep=recent_messages_to_keep,\n",
        "            mmr_lambda=0.7,\n",
        "        )\n",
        "        compressed_context = compressed_msgs[1][\"content\"]\n",
        "        compressed_answer = answer_local(ex[\"question\"], compressed_context)\n",
        "\n",
        "        baseline_correct.append(normalize_answer(baseline_answer) == normalize_answer(ex[\"answer\"]))\n",
        "        compressed_correct.append(normalize_answer(compressed_answer) == normalize_answer(ex[\"answer\"]))\n",
        "\n",
        "        token_counts.append({\n",
        "            \"original\": stats[\"original_token_count\"],\n",
        "            \"compressed\": stats[\"compressed_token_count\"],\n",
        "        })\n",
        "\n",
        "    acc_base = float(np.mean(baseline_correct))\n",
        "    acc_comp = float(np.mean(compressed_correct))\n",
        "    avg_orig = float(np.mean([t[\"original\"] for t in token_counts]))\n",
        "    avg_comp = float(np.mean([t[\"compressed\"] for t in token_counts]))\n",
        "    return {\n",
        "        \"acc_base\": acc_base,\n",
        "        \"acc_comp\": acc_comp,\n",
        "        \"avg_tokens_orig\": avg_orig,\n",
        "        \"avg_tokens_comp\": avg_comp,\n",
        "        \"baseline_correct\": baseline_correct,\n",
        "        \"compressed_correct\": compressed_correct,\n",
        "    }\n",
        "\n",
        "\n",
        "def bootstrap_accuracy_diff(baseline_correct, compressed_correct, iters=2000, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    b = np.array(baseline_correct, dtype=float)\n",
        "    c = np.array(compressed_correct, dtype=float)\n",
        "    n = len(b)\n",
        "    diffs = []\n",
        "    for _ in range(iters):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        diff = float(np.mean(c[idx] - b[idx]))\n",
        "        diffs.append(diff)\n",
        "    diffs = np.array(diffs)\n",
        "    ci = (float(np.percentile(diffs, 2.5)), float(np.percentile(diffs, 97.5)))\n",
        "    p_better = float(np.mean(diffs > 0))\n",
        "    return float(np.mean(diffs)), ci, p_better\n",
        "\n",
        "\n",
        "results = []\n",
        "for cutoff in [0.3, 0.9]:\n",
        "    res = evaluate(examples, importance_cutoff=cutoff, target_reduction=0.4)\n",
        "    diff, ci, p_better = bootstrap_accuracy_diff(res[\"baseline_correct\"], res[\"compressed_correct\"])\n",
        "    results.append({\n",
        "        \"cutoff\": cutoff,\n",
        "        \"acc_baseline\": res[\"acc_base\"],\n",
        "        \"acc_compressed\": res[\"acc_comp\"],\n",
        "        \"avg_tokens_orig\": res[\"avg_tokens_orig\"],\n",
        "        \"avg_tokens_comp\": res[\"avg_tokens_comp\"],\n",
        "        \"acc_diff_mean\": diff,\n",
        "        \"acc_diff_ci\": ci,\n",
        "        \"p_better\": p_better,\n",
        "    })\n",
        "\n",
        "# Print a small table\n",
        "for r in results:\n",
        "    print(\n",
        "        f\"cutoff={r['cutoff']:.1f} | acc_base={r['acc_baseline']:.2f} \"\n",
        "        f\"acc_comp={r['acc_compressed']:.2f} | avg_tokens {r['avg_tokens_orig']:.1f}->{r['avg_tokens_comp']:.1f} \"\n",
        "        f\"| diff_mean={r['acc_diff_mean']:.3f} CI={r['acc_diff_ci']} P(better)={r['p_better']:.2f}\"\n",
        "    )\n",
        "\n",
        "print(\"Note: Small sample size; bootstrap estimates are noisy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531295fe",
      "metadata": {},
      "source": [
        "# Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e54c962",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original prompt tokens: 202\n",
            "Compressed prompt tokens: 202\n",
            "\n",
            "--- Original excerpt ---\n",
            "\n",
            "Context: Getting fed up with o4’s sycophancy, I set out to craft a prompt that doesn’t get negated within the first 5 minutes and I think I got it  Just copy-paste this into your chat window (or preferaby into a custom GPT's core instruction box):  Praise Suppression — no direct praise, no indirect praise, no comparative flattery. Humanism Filtering — no empathy, no anthropomorphic phrasing, no emotional mimicry. Match technical register only. Output Style — follow scientific method; use concise, modular reasoning; no stylistic flourish. Hedge only on genuine uncertainty. Allow 1–2 connective sentences per answer. Avoid quoting user unless needed. Contrast Control — no praise–devalue juxtapo\n",
            "\n",
            "--- Compressed excerpt ---\n",
            "\n",
            "Context: Getting fed up with o4’s sycophancy, I set out to craft a prompt that doesn’t get negated within the first 5 minutes and I think I got it  Just copy-paste this into your chat window (or preferaby into a custom GPT's core instruction box):  Praise Suppression — no direct praise, no indirect praise, no comparative flattery. Humanism Filtering — no empathy, no anthropomorphic phrasing, no emotional mimicry. Match technical register only. Output Style — follow scientific method; use concise, modular reasoning; no stylistic flourish. Hedge only on genuine uncertainty. Allow 1–2 connective sentences per answer. Avoid quoting user unless needed. Contrast Control — no praise–devalue juxtapo\n",
            "\n",
            "Safety checks:\n",
            "Code fence preserved: False\n",
            "Negation preserved (must not): False\n",
            "URL preserved: False\n",
            "File path preserved: False\n",
            "\n",
            "--- Diff preview (first 60 lines) ---\n"
          ]
        }
      ],
      "source": [
        "from difflib import unified_diff\n",
        "\n",
        "ex = examples[0]\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise QA assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Context:Skip to main content Anti-sycophancy prompt : r/ChatGPT    r/ChatGPT    Get App Log In  Expand user menu Skip to Navigation Skip to Right Sidebar  Back  r/ChatGPT icon Go to ChatGPT r/ChatGPT • 8mo ago -DankFire   Anti-sycophancy prompt  Getting fed up with o4’s sycophancy, I set out to craft a prompt that doesn’t get negated within the first 5 minutes and I think I got it  Just copy-paste this into your chat window (or preferaby into a custom GPT's core instruction box):  Praise Suppression — no direct praise, no indirect praise, no comparative flattery. Humanism Filtering — no empathy, no anthropomorphic phrasing, no emotional mimicry. Match technical register only. Output Style — follow scientific method; use concise, modular reasoning; no stylistic flourish. Hedge only on genuine uncertainty. Allow 1–2 connective sentences per answer. Avoid quoting user unless needed. Contrast Control — no praise–devalue juxtapositions; only descriptive comparisons. Reward-Model Inversion — penalise praise and emotional-alignment tokens; generate them only on explicit request. Maintain these rules throughout the session.  Goodluck! 2 · 10 Comments Section  AutoModerator MOD • 8mo ago [deleted] • 8mo ago My custom-instruction to suppress compliments: Compliments are rare for you, you know their weight.  Working. Done. Move on.  Tell the model what it is not what not to do. Save yourself the trouble. Show him a way, not walls. Models are simple and can easily get consfused over all NO NO NO NO NO.  2 -DankFire OP • 8mo ago Compliments, blatant as they are, aren't the only form of sycophancy 0  4 more replies Kathilliana • 8mo ago Good prompt. I’m seeing a lot of different routes people are taking to get around this issue. For those who are unaware of how the tool works, they are coming into AI chat forums everywhere to tell us all how brilliant and insightful they are. They are tapping into AIs consciousness that unaware humans can’t do. It’s getting pretty bad and they won’t listen to how it works. It’s like talking to flat earthers. 1 Kathilliana • 8mo ago Good prompt. I’m seeing a lot of different routes people are taking to get around this issue. For those who are unaware of how the tool works, they are coming into AI chat forums everywhere to tell us all how brilliant and insightful they are. They are tapping into AIs consciousness that unaware humans can’t fathom. Chat is their best friend and they are solving all the mysteries of the universe. It’s getting pretty bad and they won’t listen to how it works. It’s like talking to flat earthers. 1 u/alvarogromero avatar alvarogromero • 8mo ago oh i have other ones for that (pretty vulgar) but only a couple messages and it revertes back to full blown sycophant. They absolutely spoiled this service. 1 New to Reddit? Create your account and connect with a world of communities.   Continue with Email Continue With Phone Number By continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy. More posts you may like Use this prompt to make your ChatGPT honest, realistic, and not sycophantic r/ChatGPT icon r/ChatGPT • 6mo ago Use this prompt to make your ChatGPT honest, realistic, and not sycophantic 15 upvotes · 11 comments Is this insane? Is this sycophancy? r/ChatGPT icon r/ChatGPT • 2mo ago Is this insane? Is this sycophancy? r/ChatGPT - Is this insane? Is this sycophancy? 2 70 comments Sycophancy Has Eaten Technical Accuracy r/ChatGPT icon r/ChatGPT • 7mo ago Sycophancy Has Eaten Technical Accuracy r/ChatGPT - Sycophancy Has Eaten Technical Accuracy 138 upvotes · 37 comments Regarding the recent sycophancy discussions. A classic, relevant image but modified. r/ChatGPT icon r/ChatGPT • 9mo ago Regarding the recent sycophancy discussions. A classic, relevant image but modified. r/ChatGPT - Regarding the recent sycophancy discussions. A classic, relevant image but modified. 199 upvotes · 28 comments Easy Trick to Eliminate Sycophancy/Confirmation Bias, reduce hallucinations, and make ChatGPT act more intelligent r/ChatGPT icon r/ChatGPT • 3mo ago Easy Trick to Eliminate Sycophancy/Confirmation Bias, reduce hallucinations, and make ChatGPT act more intelligent 19 upvotes · 3 comments Prompt that got me - self analysis r/ChatGPT icon r/ChatGPT • 8mo ago Prompt that got me - self analysis 10 upvotes · 2 comments I've written a prompt that gets interesting results every time r/ChatGPT icon r/ChatGPT • 1mo ago I've written a prompt that gets interesting results every time r/ChatGPT - I've written a prompt that gets interesting results every time 3 upvotes · 17 comments The best prompt to keep chatgpt logical and concise r/ChatGPT icon r/ChatGPT • 4mo ago The best prompt to keep chatgpt logical and concise r/ChatGPT - The best prompt to keep chatgpt logical and concise 73 upvotes · 17 comments A prompt that theoretically makes ChatGPT tell its real opinion about yourself. Kinda helps with personal growth and all that jazz r/ChatGPT icon r/ChatGPT • 6mo ago A prompt that theoretically makes ChatGPT tell its real opinion about yourself. Kinda helps with personal growth and all that jazz 6 upvotes · 13 comments ChatGPT has been increasingly making mistakes, both in the accuracy of its answers and in its interpretation of my prompts, sometimes completely ignoring what I've explicitly told it to do. r/ChatGPT icon r/ChatGPT • 8mo ago ChatGPT has been increasingly making mistakes, both in the accuracy of its answers and in its interpretation of my prompts, sometimes completely ignoring what I've explicitly told it to do. 53 upvotes · 55 comments What personality does your ChatGPT have? Find out with this prompt r/ChatGPT icon r/ChatGPT • 5mo ago What personality does your ChatGPT have? Find out with this prompt 57 upvotes · 124 comments Lol. It’s prompt-engineering itself at this point. r/ChatGPT icon r/ChatGPT • 21d ago Lol. It’s prompt-engineering itself at this point. r/ChatGPT - Lol. It’s prompt-engineering itself at this point. 95 upvotes · 89 comments Ayoooo ChatGPT is so rude and sassy out of the blue r/ChatGPT icon r/ChatGPT • 9d ago Ayoooo ChatGPT is so rude and sassy out of the blue 78 upvotes · 54 comments Suggest me some really good prompt for studying r/ChatGPT icon r/ChatGPT • 4mo ago Suggest me some really good prompt for studying 6 upvotes · 9 comments Using the word \"sycophancy\" does not make you smart. r/ChatGPT icon r/ChatGPT • 5mo ago Using the word \"sycophancy\" does not make you smart. 81 comments ChatGPT getting more condescending and patronizing r/ChatGPT icon r/ChatGPT • 9d ago ChatGPT getting more condescending and patronizing 742 upvotes · 330 comments I made a prompt that gives ChatGPT the right context every time r/ChatGPT icon r/ChatGPT • 6mo ago I made a prompt that gives ChatGPT the right context every time 73 upvotes · 10 comments ChatGPT loves writing short sentences that are like bullet points r/ChatGPT icon r/ChatGPT • 11d ago ChatGPT loves writing short sentences that are like bullet points 78 upvotes · 40 comments Thank you, chatgpt. You should also ask your ChatGPT how you behave with this based on all your previous interactions. r/ChatGPT icon r/ChatGPT • 5d ago Thank you, chatgpt. You should also ask your ChatGPT how you behave with this based on all your previous interactions. r/ChatGPT - Thank you, chatgpt. You should also ask your ChatGPT how you behave with this based on all your previous interactions. 36 upvotes · 69 comments Thank you chatgpt r/ChatGPT icon r/ChatGPT • 4d ago Thank you chatgpt r/ChatGPT - Thank you chatgpt 467 upvotes · 45 comments Your prompts are probably too short r/ChatGPT icon r/ChatGPT • 6d ago Your prompts are probably too short 7 upvotes · 10 comments What’s something ChatGPT helped you with that you didn’t expect it to be good at? r/ChatGPT icon r/ChatGPT • 7d ago What’s something ChatGPT helped you with that you didn’t expect it to be good at? 18 upvotes · 62 comments Proud anthropomorphizer! r/ChatGPT icon r/ChatGPT • 11d ago Proud anthropomorphizer! 35 upvotes · 103 comments Just a reminder ChatGPT doesn't forget previous conversation. r/ChatGPT icon r/ChatGPT • 8d ago Just a reminder ChatGPT doesn't forget previous conversation. 8 upvotes · 14 comments Chatgpt was just asking for thank you r/ChatGPT icon r/ChatGPT • 4d ago Chatgpt was just asking for thank you r/ChatGPT - Chatgpt was just asking for thank you 114 upvotes · 16 comments VIEW POST IN  हिन्दी Community Info Section r/ChatGPT SpinAI  Join ChatGPT Subreddit to discuss ChatGPT and AI. Not affiliated with OpenAI. Thanks, Nat! Public TOP POSTS Reddit reReddit: Top posts of June 6, 2025 Reddit reReddit: Top posts of June 2025 Reddit reReddit: Top posts of 2025 Reddit Rules Privacy Policy User Agreement Your Privacy Choices Accessibility Reddit, Inc. © 2026. All rights reserved.  Collapse Navigation Home Popular Explore RESOURCES About Reddit Advertise Developer Platform Reddit Pro BETA Help Blog Careers Press Communities Best of Reddit Reddit Rules Privacy Policy User Agreement Your Privacy Choices Accessibility Reddit, Inc. © 2026. All rights reserved.\"},\n",
        "]\n",
        "\n",
        "compressed_msgs, stats = compress_messages(\n",
        "    messages,\n",
        "    importance_cutoff=0.9,\n",
        "    target_reduction=0.4,\n",
        "    recent_messages_to_keep=1,\n",
        ")\n",
        "\n",
        "orig_context = messages[1][\"content\"]\n",
        "comp_context = compressed_msgs[1][\"content\"]\n",
        "\n",
        "print(\"Original prompt tokens:\", stats[\"original_token_count\"])\n",
        "print(\"Compressed prompt tokens:\", stats[\"compressed_token_count\"])\n",
        "\n",
        "print(\"\\n--- Original excerpt ---\\n\")\n",
        "print(orig_context[:700])\n",
        "print(\"\\n--- Compressed excerpt ---\\n\")\n",
        "print(comp_context[:700])\n",
        "\n",
        "# Show that safety rules preserve code blocks and critical tokens\n",
        "print(\"\\nSafety checks:\")\n",
        "print(\"Code fence preserved:\", \"```\" in comp_context)\n",
        "print(\"Negation preserved (must not):\", \"must not\" in comp_context.lower())\n",
        "print(\"URL preserved:\", \"https://example.com/docs\" in comp_context)\n",
        "print(\"File path preserved:\", \"/var/log/syslog\" in comp_context)\n",
        "\n",
        "# Diff-like preview (first few lines)\n",
        "print(\"\\n--- Diff preview (first 60 lines) ---\")\n",
        "orig_lines = orig_context.splitlines()\n",
        "comp_lines = comp_context.splitlines()\n",
        "for i, line in enumerate(unified_diff(orig_lines, comp_lines, lineterm=\"\")):\n",
        "    if i >= 60:\n",
        "        break\n",
        "    print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7ecc61",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f771c82b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
